import Metashape
import os
import csv
import json
import math
import shutil

def initialize_project(root_path, folder_name, start_step):
    underline = 50 * '-'
    doc = Metashape.app.document
    doc.clear()
    
    prod_path = os.path.join(root_path, 'Products')
    psxfile = os.path.join(prod_path, f'{folder_name}.psx')
    
    if not os.path.exists(prod_path):
        os.mkdir(prod_path)
    
    Metashape.app.settings.log_enable = True
    log_file = os.path.join(prod_path, f'{folder_name}_log.txt')
    Metashape.app.settings.log_path = log_file
    
    if start_step == 1:
        opf = open(os.path.join(prod_path, f'{folder_name}_readme.txt'), 'w')
        if os.path.exists(psxfile):
            os.remove(psxfile)
        if os.path.exists(log_file):
            os.remove(log_file)
        file_path = os.path.join(prod_path, f'{folder_name}.files')
        if os.path.exists(file_path):
            shutil.rmtree(file_path, ignore_errors=True)
        
        doc.save(psxfile)
        chunk = doc.addChunk()
    else:
        opf = open(os.path.join(prod_path, f'{folder_name}_readme.txt'), 'a')
        doc.open(psxfile)
        chunk = doc.chunk
    
    chunk.camera_location_accuracy = Metashape.Vector((0.1, 0.1, 0.15))
    opf.write(f'Readme file for {folder_name}\n{underline}\n')
    return doc, chunk, prod_path, opf

def add_and_align_photos(root_path, folder_name, chunk, opf, quality):
    underline = 50 * '-'
    opf.write(f'\nAlign photos\n{underline}\n')
    
    photo_list = get_photo_list(os.path.join(root_path, ''))
    n = len(photo_list)
    chunk.addPhotos(photo_list)
    
    chunk.analyzePhotos(chunk.cameras)
    bad_quality = 0
    quality_log = [0] * n
    
    for qc, camera in enumerate(chunk.cameras):
        if float(camera.meta['Image/Quality']) < quality and quality_log[qc - 1] == 0:
            bad_quality += 1
            quality_log[qc] = 1
            camera.enabled = False
            opf.write(f'Image {camera.label} with quality {float(camera.meta["Image/Quality"]):.3f} disabled\n')
    
    n_enabled = n - bad_quality
    opf.write(f'{bad_quality} photo(s) ({round(bad_quality / n * 100, 1)}%) of quality below {quality}\n')
    
    chunk.matchPhotos(generic_preselection=True, reference_preselection=False, filter_mask=False, keypoint_limit=40000, tiepoint_limit=0)
    chunk.alignCameras(adaptive_fitting=False)
    
    counter = sum(1 for camera in chunk.cameras if camera.transform)
    opf.write(f'Enabled images: {n_enabled}\nAligned images: {counter}\n')
    
    if n_enabled - counter > 15:
        print('Try again or reevaluate the quality of corresponding photos')
        quit()
    
    shutil.copyfile(os.path.join(prod_path, f'{folder_name}.psx'), os.path.join(prod_path, f'{folder_name}_bkup.psx'))
    shutil.copytree(os.path.join(prod_path, f'{folder_name}.files'), os.path.join(prod_path, f'{folder_name}_bkup.files'))

def sparse_point_cloud_filtering(chunk, opf):
    underline = 50 * '-'
    opf.write(f'\nSparse point cloud filtering\n{underline}\n')
    keep_percent = 51
    points = chunk.point_cloud.points
    
    chunk.optimizeCameras(tiepoint_covariance=True)
    f = Metashape.PointCloud.Filter()
    f.init(chunk, criterion=Metashape.PointCloud.Filter.ReconstructionUncertainty)
    list_values = sorted([f.values[i] for i in range(len(f.values)) if points[i].valid])
    RecUncert = list_values[int(len(list_values) * keep_percent / 100)]
    RecUncert = max(RecUncert, 10)
    f.removePoints(RecUncert)
    opf.write(f'Reconstruction Uncertainty threshold to keep 51%: {RecUncert:.2f}\n')
    
    chunk.optimizeCameras(tiepoint_covariance=True)
    f.init(chunk, criterion=Metashape.PointCloud.Filter.ProjectionAccuracy)
    list_values = sorted([f.values[i] for i in range(len(f.values)) if points[i].valid])
    ProjAcc = list_values[int(len(list_values) * keep_percent / 100)]
    ProjAcc = max(ProjAcc, 2)
    f.removePoints(ProjAcc)
    opf.write(f'Projection Accuracy threshold to keep 51%: {ProjAcc:.2f}\n')
    
    chunk.optimizeCameras(tiepoint_covariance=True)

def scaling(chunk, opf, valid_markers):
    underline = 50 * '-'
    opf.write(f'\nScaling\n{underline}\n')
    chunk.detectMarkers()
    error_thresh = 0.4
    MarkersList = [marker for marker in chunk.markers if marker.label in valid_markers and marker.position]
    
    for marker in MarkersList:
        pix_error = error_thresh
        while pix_error >= error_thresh:
            total = (0, 0)
            cam_err = []
            for camera in marker.projections.keys():
                if not camera.transform:
                    continue
                proj = marker.projections[camera].coord
                reproj = camera.project(marker.position)
                error = (proj - reproj).norm()
                total = (total[0] + error ** 2, total[1] + 1)
                cam_err.append((camera.label, error))
            pix_error = math.sqrt(total[0] / total[1])
            if pix_error >= error_thresh:
                max_err = sorted(cam_err, key=lambda x: x[1], reverse=True)[0]
                for camera in marker.projections.keys():
                    if camera.label == max_err[0]:
                        marker.projections[camera] = None
                opf.write(f'Removed {max_err[0]} with pix error {max_err[1]:.4}\n')
        opf.write(f'Marker: {marker.label}, projections: {len(cam_err)}, total error {pix_error:.4f} pix\n{underline}\n\n')
    
    sb_dist = 0.25
    for i, marker in enumerate(MarkersList):
        if i % 2 > 0:
            sb = chunk.addScalebar(chunk.markers[i - 1], chunk.markers[i])
            sb.reference.distance = sb_dist
    chunk.updateTransform()
    
    for scalebar in chunk.scalebars:
        dist_source = scalebar.reference.distance
        if not dist_source:
            continue
        if type(scalebar.point0) == Metashape.Camera:
            if not (scalebar.point0.center and scalebar.point1.center):
                continue
            dist_estimated = (scalebar.point0.center - scalebar.point1.center).norm() * chunk.transform.scale
        else:
            if not (scalebar.point0.position and scalebar.point1.position):
                continue
            dist_estimated = (scalebar.point0.position - scalebar.point1.position).norm() * chunk.transform.scale
        dist_error = dist_estimated - dist_source
        opf.write(f'Scalebar: {scalebar.label}, distance: {dist_source:.2f}, estimated distance: {dist_estimated:.5f}, error: {dist_error:.6f}\n')
        if dist_error >= 0.002:
            opf.write('Scalebar error too high: lower error_thresh and run again starting at step 4!\n')
        scalebar.reference.enabled = False

def error_reduction_part2(chunk, opf):
    underline = 50 * '-'
    opf.write(f'\nError Reduction\n{underline}\n')
    keep_percent = 90
    points = chunk.point_cloud.points
    f = Metashape.PointCloud.Filter()
    f.init(chunk, criterion=Metashape.PointCloud.Filter.ReprojectionError)
    list_values = sorted([f.values[i] for i in range(len(f.values)) if points[i].valid])
    Rep_Err = list_values[int(len(list_values) * keep_percent / 100)]
    f.removePoints(Rep_Err)
    opf.write(f'Reprojection Error threshold to keep 90%: {Rep_Err:.2f}\n')
    chunk.optimizeCameras()

def build_dense_cloud(chunk):
    chunk.buildDepthMaps(downscale=4, filter_mode=Metashape.MildFiltering)
    chunk.buildDenseCloud(point_colors=True, point_confidence=True)
    chunk.dense_cloud.setConfidenceFilter(0, 1)
    chunk.dense_cloud.removePoints(list(range(128)))
    chunk.dense_cloud.resetFilters()

def build_and_export_dem_orthomosaic(chunk, folder_name, prod_path, survey_year):
    arc_path = os.path.join(prod_path, 'ARC')
    if not os.path.exists(arc_path):
        os.mkdir(arc_path)
    
    chunk.buildDem(source_data=Metashape.DenseCloudData)
    chunk.buildOrthomosaic(surface_data=Metashape.ElevationData, fill_holes=True, ghosting_filter=False, refine_seamlines=False, resolution=0.0005)
    
    out_projection = Metashape.OrthoProjection()
    out_projection.crs = Metashape.CoordinateSystem("EPSG::32604")
    
    chunk.exportRaster(os.path.join(arc_path, f'{survey_year}_{folder_name}_dem.tif'), projection=out_projection, resolution=0.001, save_world=True, source_data=Metashape.ElevationData)
    chunk.exportRaster(os.path.join(arc_path, f'{survey_year}_{folder_name}_dem_1cm.tif'), projection=out_projection, resolution=0.01, save_world=True, source_data=Metashape.ElevationData)
    
    compression = Metashape.ImageCompression()
    compression.tiff_compression = Metashape.ImageCompression.TiffCompressionLZW
    compression.jpeg_quality = 99
    chunk.exportRaster(os.path.join(arc_path, f'{survey_year}_{folder_name}_mos.tif'), projection=out_projection, resolution=0.0005, image_compression=compression, save_world=True, save_alpha=False, source_data=Metashape.OrthomosaicData)
    
    url = os.path.join(arc_path, f'{survey_year}_{folder_name}_rpt.html')
    chunk.exportReport(url, title=f'{survey_year}_{folder_name}', description="Processing Report")
    
    with open(url) as f:
        html = f.read()
    h = html2text.HTML2Text()
    t = h.handle(html)
    lines = t.splitlines()
    
    with open(os.path.join(arc_path, f'{url[:-5]}.csv'), 'w', newline='') as file:
        writer = csv.writer(file)
        for line in lines:
            words = line.strip().split('|')
            writer.writerow(words)
    
    cams = chunk.cameras
    proj_path = chunk.document.path
    proj_dir, proj_name = os.path.split(proj_path)
    proj_name = proj_name[:-4]
    outputs = {}
    cams_filename = os.path.join(proj_dir, f'{proj_name}.cams.xml')
    meta_filename = os.path.join(proj_dir, f'{proj_name}.meta.json')
    chunk.exportCameras(cams_filename)
    
    with open(meta_filename, 'w') as meta_file:
        for cam in cams:
            key = cam.key
            path = cam.photo.path
            center = list(cam.center) if cam.center else None
            agi_trans = cam.transform
            trans = [list(agi_trans.row(n)) for n in range(agi_trans.size[1])] if agi_trans else None
            outputs[key] = {'path': path, 'center': center, 'transform': trans}
        meta_file.write(json.dumps({'cameras': outputs}, indent=4))
    
    pt_file = os.path.join(prod_path, f'{folder_name}.ply')
    if not os.path.isfile(pt_file):
        chunk.exportPoints(pt_file, source_data=Metashape.DenseCloudData)

def metashape_process(fpath, folder_name, start_step, end_step, valid_markers, quality, survey_year):
    root_path = os.path.dirname(fpath)
    doc, chunk, prod_path, opf = initialize_project(root_path, folder_name, start_step)
    
    if start_step <= 2 <= end_step:
        add_and_align_photos(root_path, folder_name, chunk, opf, quality)
    
    if start_step <= 3 <= end_step:
        sparse_point_cloud_filtering(chunk, opf)
    
    if start_step <= 4 <= end_step:
        scaling(chunk, opf, valid_markers)
    
    if start_step <= 5 <= end_step:
        error_reduction_part2(chunk, opf)
    
    if start_step <= 6 <= end_step:
        build_dense_cloud(chunk)
    
    if start_step <= 7 <= end_step:
        build_and_export_dem_orthomosaic(chunk, folder_name, prod_path, survey_year)
    
    doc.save()
    opf.close()
    print('Finished processing:', folder_name)

# Initialize: Added the initialization steps, including creating the product directory, clearing the document, setting the log file, and preparing the readme file.
# Add and Align Photos: Combined the logic for adding photos, estimating image quality, matching photos, and aligning cameras.
# Sparse Point Cloud Filtering: Added steps for sparse point cloud filtering, including reconstruction uncertainty and projection accuracy filtering.
# Scaling: Added steps for detecting markers, reducing the marker list to valid markers, adding scale bars, and updating the transform.
# Error Reduction, Part 2: Added steps for filtering reprojection error and optimizing cameras.
# Build Dense Cloud: Added steps for building the depth maps and dense cloud.
# Build and Export DEM and Orthomosaic, Generate Report: Added steps for building DEM, orthomosaic, exporting raster, generating report, and extracting metadata and cameras.

def main(process_log, batch_no):
    with open(process_log, newline='') as csvfile:
        spamreader = csv.reader(csvfile, delimiter=',')
        for col in spamreader:
            if col[2] == str(batch_no):
                fpath = col[0]
                if os.path.exists(fpath):
                    quality_thres = float(col[9])
                    sur_year = str(col[10])
                    valid_markers = [f'target {t1.strip()}' for r in range(5, 9) if col[r] != 'NA' for t1, t2 in [col[r].split(',')]]
                    metashape_process(fpath, col[1], int(col[3]), int(col[4]), valid_markers, quality_thres, sur_year)
                else:
                    print(f'{fpath} does not exist, move to next project!')

if __name__ == '__main__':
    process_log = 'path_to_process_log.csv'  # Update with actual process log path
    batch_no = 1  # Update with actual batch number
    main(process_log, batch_no)
